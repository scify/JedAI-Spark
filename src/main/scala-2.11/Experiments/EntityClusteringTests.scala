package Experiments

import java.util.Calendar

import SparkER.DataStructures.{Profile, WeightedEdge}
import SparkER.EntityClustering.{ConnectedComponentsClustering, CutClustering, EntityClusterUtils}
import SparkER.SimJoins.Commons.CommonFunctions
import SparkER.SimJoins.SimJoins.PPJoin
import SparkER.Wrappers.CSVWrapper
import org.apache.log4j.{FileAppender, Level, LogManager, SimpleLayout}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Tests the clustering methods
  */
object EntityClusteringTests {

  def main(args: Array[String]): Unit = {

    /*
    * First: we need to obtain the scored pairs of entity profiles, we can use any of the proposed methods.
    * Here we employ PPJOIN.
    * */

    /* Dataset to test */
    val dataset = "census"

    /* Base path where the dataset is located */
    val basePath = "datasets/dirty/" + dataset + "/"

    /* Profiles to join */
    val filePath = basePath + dataset + ".csv"

    /* Groundtruth */
    val gtPath = basePath + dataset + "_groundtruth.csv"

    /** Log file */
    val logPath = "log.txt"

    /** Spark configuration */
    val conf = new SparkConf()
      .setAppName("Main")
      .setMaster("local[*]")
      .set("spark.default.parallelism", "4")

    val sc = new SparkContext(conf)

    val log = LogManager.getRootLogger
    log.setLevel(Level.INFO)
    val layout = new SimpleLayout()
    val appender = new FileAppender(layout, logPath, false)
    log.addAppender(appender)

    /* Loads the profiles and extracts all fields as text documents */
    val profiles = CSVWrapper.loadProfiles2(filePath, realIDField = "id", header = true)
    val maxProfileId = profiles.map(_.id).max()

    val docs = CommonFunctions.extractAllFields(profiles)
    docs.cache()
    val nd = docs.count()
    log.info("[PPJoin] Number of docs " + nd)

    /** Performs the join, keeping all the pairs with a similarity >= threshold */
    val t1 = Calendar.getInstance().getTimeInMillis
    val threshold = 0.2
    val matches = PPJoin.getMatches(docs, threshold)
    matches.cache()
    val nm = matches.count()
    docs.unpersist()

    val t2 = Calendar.getInstance().getTimeInMillis

    log.info("[PPJoin] Join+verification time (s) " + (t2 - t1) / 1000.0)
    log.info("[PPJoin] Number of matches " + nm)

    /** Loads the groundtruth */
    val groundtruth = CSVWrapper.loadGroundtruth(gtPath)

    /** Converts the ids in the groundtruth to the autogenerated ones */
    val realIdIds = sc.broadcast(profiles.map { p =>
      (p.originalID, p.id)
    }.collectAsMap())

    var newGT: Set[(Int, Int)] = null
    newGT = groundtruth.map { g =>
      val first = realIdIds.value.get(g.firstEntityID)
      val second = realIdIds.value.get(g.secondEntityID)
      if (first.isDefined && second.isDefined) {
        val f = first.get
        val s = second.get
        if (f < s) (f, s) else (s, f)
      }
      else {
        (-1, -1)
      }
    }.filter(_._1 >= 0).collect().toSet

    /** Now we have the candidate pairs, we can create the edges for the clustering methods */
    val edges = matches.map(x => WeightedEdge(x._1, x._2, x._3))


    /** Pick up one */

    //val cc1 = ConnectedComponentsClustering.getClusters(profiles = profiles, edges, maxProfileId, edgesThreshold = 0.5)
    //val cc1 = CenterClustering.getClusters(profiles = profiles, edges, maxProfileId, edgesThreshold = 0.5)
    //val cc1 = MergeCenterClustering.getClusters(profiles = profiles, edges, maxProfileId, edgesThreshold = 0.5)
    //val cc1 = MarkovClustering.getClusters(profiles = profiles, edges, maxProfileId, edgesThreshold = 0.5, separatorID = -1)
    //val cc1 = RicochetSRClustering.getClusters(profiles = profiles, edges, maxProfileId, edgesThreshold = 0.5, separatorID = -1)
    val cc1 = CutClustering.getClusters(profiles = profiles, edges, maxProfileId, edgesThreshold = 0.3, separatorID = -1)

    println("Number of clusters " + cc1.count())
    println("Number of profiles " + cc1.flatMap(_._2).count())
    println("Recall, precision: " + EntityClusterUtils.calcPcPqCluster(cc1, sc.broadcast(newGT)))
  }


}
