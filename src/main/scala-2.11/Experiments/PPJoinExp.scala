package Experiments

import java.util.Calendar

import SparkER.DataStructures.WeightedEdge
import SparkER.EntityClustering.{ConnectedComponentsClustering, EntityClusterUtils}
import SparkER.SimJoins.Commons.CommonFunctions
import SparkER.SimJoins.SimJoins.{EDJoin, PPJoin}
import SparkER.Wrappers.JSONWrapper
import org.apache.log4j.{FileAppender, Level, LogManager, SimpleLayout}
import org.apache.spark.{SparkConf, SparkContext}

object PPJoinExp {
  def main(args: Array[String]): Unit = {


    val dataset = "300K"

    val basePath = "C:\\Users\\gagli\\Downloads\\syntheticDatasets\\syntheticDatasets\\"

    val filePath = basePath + dataset + "profiles.json"
    val gtPath = basePath + dataset + "IdDuplicates.json"

    val logPath = "C:/Users/gagli/Desktop/ACMlog.txt"

    val conf = new SparkConf()
      .setAppName("Main")
      .setMaster("local[*]")
      .set("spark.default.parallelism", "4")
      .set("spark.local.dir", "/data2/tmp")

    val sc = new SparkContext(conf)

    val log = LogManager.getRootLogger
    log.setLevel(Level.INFO)
    val layout = new SimpleLayout()
    val appender = new FileAppender(layout, logPath, false)
    log.addAppender(appender)

    val profiles = JSONWrapper.loadProfiles(filePath, realIDField = "realProfileID")

    val docs = CommonFunctions.extractAllFields(profiles)
    docs.cache()
    val nd = docs.count()
    log.info("[PPJoin] Number of docs " + nd)

    val t1 = Calendar.getInstance().getTimeInMillis
    val matches = PPJoin.getMatches(docs, 0.25)
    matches.cache()
    val nm = matches.count()
    docs.unpersist()

    val t2 = Calendar.getInstance().getTimeInMillis

    log.info("[PPJoin] Join+verification time (s) " + (t2 - t1) / 1000.0)
    log.info("[PPJoin] Number of matches " + nm)


    val clusters = ConnectedComponentsClustering.getClusters(profiles, matches.map(x => WeightedEdge(x._1, x._2, 0)), 0)
    clusters.cache()
    val cn = clusters.count()
    val t3 = Calendar.getInstance().getTimeInMillis
    log.info("[PPJoin] Number of clusters " + cn)
    log.info("[PPJoin] Clustering time (s) " + (t3 - t2) / 1000.0)

    log.info("[PPJoin] Total time (s) " + (t3 - t1) / 1000.0)

    val groundtruth = JSONWrapper.loadGroundtruth(gtPath, firstDatasetAttribute = "d1Id", secondDatasetAttribute = "d2Id")

    //Converts the ids in the groundtruth to the autogenerated ones
    val realIdIds = sc.broadcast(profiles.map { p =>
      (p.originalID, p.id)
    }.collectAsMap())

    var newGT: Set[(Int, Int)] = null
    newGT = groundtruth.map { g =>
      val first = realIdIds.value.get(g.firstEntityID)
      val second = realIdIds.value.get(g.secondEntityID)
      if (first.isDefined && second.isDefined) {
        val f = first.get
        val s = second.get
        if (f < s) (f, s) else (s, f)
      }
      else {
        (-1, -1)
      }
    }.filter(_._1 >= 0).collect().toSet


    log.info("[PPJoin] Groundtruth size " + groundtruth.count())
    log.info("[PPJoin] New groundtruth size " + newGT.size)

    val gt = sc.broadcast(newGT)


    val pcpq = EntityClusterUtils.calcPcPqCluster(clusters, gt)
    log.info("[PPJoin] PC " + pcpq._1)
    log.info("[PPJoin] PQ " + pcpq._2)

    val f1 = 2 * ((pcpq._1 * pcpq._2) / (pcpq._1 + pcpq._2))
    log.info("[PPJoin] F1 " + f1)
  }
}
