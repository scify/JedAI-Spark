package Experiments

import java.util.Calendar

import SparkER.DataStructures.WeightedEdge
import SparkER.EntityClustering.{ConnectedComponentsClustering, EntityClusterUtils}
import SparkER.SimJoins.Commons.CommonFunctions
import SparkER.SimJoins.SimJoins.PPJoin
import SparkER.Wrappers.CSVWrapper
import org.apache.log4j.{FileAppender, Level, LogManager, SimpleLayout}
import org.apache.spark.{SparkConf, SparkContext}

/**
  * Tests the PPJOIN implementation
  **/
object PPJoinExp {
  def main(args: Array[String]): Unit = {
    /* Dataset to test */
    val dataset = "census"

    /* Base path where the dataset is located */
    val basePath = "datasets/dirty/" + dataset + "/"

    /* Profiles to join */
    val filePath = basePath + dataset + ".csv"

    /* Groundtruth */
    val gtPath = basePath + dataset + "_groundtruth.csv"

    /** Log file */
    val logPath = "log.txt"

    /** Spark configuration */
    val conf = new SparkConf()
      .setAppName("Main")
      .setMaster("local[*]")
      .set("spark.default.parallelism", "4")

    val sc = new SparkContext(conf)

    val log = LogManager.getRootLogger
    log.setLevel(Level.INFO)
    val layout = new SimpleLayout()
    val appender = new FileAppender(layout, logPath, false)
    log.addAppender(appender)

    /* Loads the profiles and extracts all fields as text documents */
    val profiles = CSVWrapper.loadProfiles2(filePath, realIDField = "id", header = true)

    val docs = CommonFunctions.extractAllFields(profiles)
    docs.cache()
    val nd = docs.count()
    log.info("[PPJoin] Number of docs " + nd)

    /** Performs the join, keeping all the pairs with a similarity >= threshold */
    val t1 = Calendar.getInstance().getTimeInMillis
    val threshold = 0.2
    val matches = PPJoin.getMatches(docs, threshold)
    matches.cache()
    val nm = matches.count()
    docs.unpersist()

    val t2 = Calendar.getInstance().getTimeInMillis

    log.info("[PPJoin] Join+verification time (s) " + (t2 - t1) / 1000.0)
    log.info("[PPJoin] Number of matches " + nm)

    /** Performs the transitive closure */
    val clusters = ConnectedComponentsClustering.getClusters(profiles, matches.map(x => WeightedEdge(x._1, x._2, 0)), 0)
    clusters.cache()
    val cn = clusters.count()
    val t3 = Calendar.getInstance().getTimeInMillis
    log.info("[PPJoin] Number of clusters " + cn)
    log.info("[PPJoin] Clustering time (s) " + (t3 - t2) / 1000.0)

    log.info("[PPJoin] Total time (s) " + (t3 - t1) / 1000.0)

    /** Loads the groundtruth */
    val groundtruth = CSVWrapper.loadGroundtruth(gtPath)

    /** Converts the ids in the groundtruth to the autogenerated ones */
    val realIdIds = sc.broadcast(profiles.map { p =>
      (p.originalID, p.id)
    }.collectAsMap())

    var newGT: Set[(Int, Int)] = null
    newGT = groundtruth.map { g =>
      val first = realIdIds.value.get(g.firstEntityID)
      val second = realIdIds.value.get(g.secondEntityID)
      if (first.isDefined && second.isDefined) {
        val f = first.get
        val s = second.get
        if (f < s) (f, s) else (s, f)
      }
      else {
        (-1, -1)
      }
    }.filter(_._1 >= 0).collect().toSet

    log.info("[PPJoin] Groundtruth size " + groundtruth.count())
    log.info("[PPJoin] New groundtruth size " + newGT.size)

    /** Computes precision and recall */
    val gt = sc.broadcast(newGT)

    val pcpq = EntityClusterUtils.calcPcPqCluster(clusters, gt)
    log.info("[PPJoin] PC (recall) " + pcpq._1)
    log.info("[PPJoin] PQ (precision) " + pcpq._2)

    val f1 = 2 * ((pcpq._1 * pcpq._2) / (pcpq._1 + pcpq._2))
    log.info("[PPJoin] F1 " + f1)
  }
}
